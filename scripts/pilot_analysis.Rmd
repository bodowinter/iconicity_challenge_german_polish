---
title: "Analysis of Crosslinguistic Vocal Iconicity Challenge"
author: "Bodo Winter (analysis only)"
date: "11/3/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is the analysis of the Polish, German, English, and Japanese data for vocal iconicity challenge, to be presented at the ICLC conference. This is a pilot analysis that will serve as the basis for the other languages to come. After we've settled on an analysis for this data, we will preregister the main analysis for the other languages.

Load libraries:

```{r libs, message = FALSE}
library(tidyverse) # for data carpentry
library(stringr) # for string processing
library(lme4) # for mixed models
library(afex) # for likelihood ratio test, fixed effects
library(RLRsim) # for likelihood ratio tests, random effects, not used yet
library(brms) # for bayesian models, not used yet
```

Load data (column names are stored separately):

```{r load_data, message = FALSE, warning = FALSE}
# Load the main data:

df <- read_delim('../data/experimentInputs_2018-10-24.txt',
                 delim = '\t', col_names = FALSE)

# Load column names and append them:

these_cols <- readLines('../data/header.txt')
colnames(df) <- these_cols

# Show first few columns:

df %>% print(n = 2, width = Inf)
```

Check how many data points there are per experiment:

```{r exp_stats}
table(df$Experiment)
```

The "Session" column can be used as a unique identifier for subjects. Let's rename this for transparency:

```{r rename_subs}
df <- rename(df,
             Subject = Session)
```

Check how many subjects there are per experiment. 

```{r sub_stats}
length(unique(df$Subject))
```

254 subjects!

How many subjects per language?

```{r sub_per_lang}
# apply(table(df$Subject, df$Experiment),
      # MARGIN = 2, FUN = function(x) sum(x != 0))
df %>% count(Experiment) %>% mutate(n = n / 104)
```

How many women and men, per language, and overall?

```{r gender_stats}
# Overall raw counts:

table(df$Sex) / 104

# Overall percentage:

prop.table(table(df$Sex) / 104)

# Per language, raw counts:

df %>% count(Experiment, Sex) %>% mutate(n = n / 104)
```

## Compute accuracy, check claps & clean data:

The input and expected columns need to be matched for determining accuracy:

```{r compute_accuracy}
df <- mutate(df, ACC = ifelse(Input == Expected, 1, 0))
head(df$ACC)
```

Check accuracy of the clapping sound:

```{r clap_accuracy}
str_c(round(mean(filter(df, Audio == 'clapping.wav')$ACC), 2) * 100, '%')
```

97% correct across the board.

Check accuracy of the clapping sound across the four languages:

```{r clap_accuracy_avgs}
df %>% filter(Audio == 'clapping.wav') %>% 
  group_by(Experiment) %>% 
  summarize(ACC = mean(ACC))
```

Japanese the worst (96%), closely followed by German (97%) ... these differences are quite negligible. Participants seem to have performed equally well across the four language groups in terms of detecting the clapping.

Check accuracy of the clapping sound per participant:

```{r clap_accuracy_per_sub}
# Aggregate clapping accuracy per subject:

subs_clap_accuracy <- df %>%
  filter(Audio == 'clapping.wav') %>% 
  group_by(Subject) %>% 
  summarize(ACC = mean(ACC))

# Summarize clapping accuracy per subject:

subs_clap_accuracy %>% count(ACC)
```

Almost all subjects performed well with respect to the clapping sound. 228 scored perfectly, which is...

```{r percentage_claps}
str_c(round(228 / (nrow(df) / 104), 2) * 100, '%')
```

90% of them scored perfectly. There's also a bunch of people who have 90% of all claps correctly. Let's exclude those that performed less than 80% correctly. First, find the subjects:

```{r get_bad_subs}
bad_subs <- filter(subs_clap_accuracy, ACC < 0.8) %>%
  pull(Subject)

# Check:

bad_subs
length(bad_subs)
```

So, there'll be a total of 10 subjects excluded for performing badly on the clapping sound. Exclude them:

```{r exclude_bad_subs}
df <- filter(df, !(Subject %in% bad_subs))

# Check new number of participants:

nrow(df) / 104
```

Now we can get rid of the clapping sounds (which shouldn't be included in the overall accuracies):

```{r exclude_claps}
df <- filter(df, Audio != 'clapping.wav')
```

Next, let's get rid of the kiki/bouba/r/l/ stims:

```{r exclude_kiki_rl}
not_these <- c('kiki.wav', 'bouba.wav', 'l.wav', 'r.wav')
df <- filter(df, !(Audio %in% not_these))
```

How many unique data points do we have now? (these are actually the experimental items)

```{r unique_data_sum}
nrow(df)
```

Now that we have only the relevant trials, we can separate the team and item info, which is contained in the audio file:

```{r clean_items}
df <- separate(df, Audio,
               into = c('Team', 'Item', 'Extension')) %>%
  select(-Extension)
```

## Descriptive averages

Check descriptive averages per languages:

```{r per_lang_ACC}
df %>% group_by(Experiment) %>% 
  summarize(ACC = mean(ACC)) %>%
  mutate(Percentage = str_c(round(ACC, 2) * 100, '%'))
```

Check descriptive averages per items, sorted from best to worst:

```{r per_item_ACC}
df %>% group_by(Item) %>% 
  summarize(ACC = mean(ACC)) %>%
  mutate(Percentage = str_c(round(ACC, 2) * 100, '%')) %>% arrange(desc(ACC)) %>%
  print(n = Inf)
```

Check descriptive averages per team, sorted from best to worst:

```{r per_team_ACC}
df %>% group_by(Team) %>% 
  summarize(ACC = mean(ACC)) %>%
  mutate(Percentage = str_c(round(ACC, 2) * 100, '%')) %>% arrange(desc(ACC)) %>%
  print(n = Inf)
```

Include concept codes. First load them:

```{r per_concept, message = FALSE}
concepts <- read_csv('../data/concept_codes.csv')
concepts
```

Then merge concept info into main data frame:

```{r merge_concepts}
df <- left_join(df, concepts,
                by = c('Item' = 'meaning'))
```

Check accuracy across per category:

```{r concept_ACC}
df %>% group_by(category) %>%
  summarize(ACC = mean(ACC)) %>% 
  mutate(Percentage = str_c(round(ACC, 2) * 100, '%')) %>%
  arrange(desc(ACC))
```

Check accuracy across per subcategory:

```{r subconcept_ACC}
df %>% group_by(subCategory) %>%
  summarize(ACC = mean(ACC)) %>% 
  mutate(Percentage = str_c(round(ACC, 2) * 100, '%')) %>%
  arrange(desc(ACC))
```

## Inferential stats (preliminary)

This is testing intercepts against a log odd of 0 (= 50% chance), which is an even more conservative criterion since chance was actually at 16%.

This is an intercept-only model with four random intercepts, one for subjects, experiments (=languages in this case), items, and team.

```{r ACC_lme4, cache = TRUE}
xmdl <- glmer(ACC ~ 1 +
                (1|Subject) + (1|Experiment) +
                (1|Item) + (1|Team),
              data = df, family = binomial)
summary(xmdl)
```

Things to note in this model: The intercept is significant (Wald's Z test), which means overall performance is above 50%. By quite a bit.

In fact, the odds of observing a correct response as opposed to an incorrect response are:

```{r odds}
exp(fixef(xmdl))
```

They are 2.5 to 1. In terms of percentages, the model estimates:

```{r percentages}
plogis(fixef(xmdl))
```

... on average 72% correct.

Another thing to note about the model is the random effect variation:

```{r model_RE}
summary(xmdl)$varcor
```

There is more random effect variation due to items than due to subjects, teams or languages! In fact, there is more variation due to subjects than there is due to languages!! This suggests quite stunning cross-linguistic uniformity.

Finally, let's perform a likelihood ratio test of concept categories. I'm omitting all random slopes for now, which is not optimal. In a Bayesian model we'll be able to fit all the necessary random slopes more easily.

```{r LRT_category, cache = TRUE}
xmdl_concept <- mixed(ACC ~ category +
                (1|Subject) + (1|Experiment) +
                (1|Item) + (1|Team),
              data = df, family = binomial,
              method = 'LRT')
xmdl_concept
```

That's quite borderline significant! I wouldn't bet on the category effect in this model yet â€” especially since there are no random slopes for category added to the model. But with more power in the final analysis, this will be worth testing.

```{r LRT_sub_category, cache = TRUE}
xmdl_sub_concept <- mixed(ACC ~ subCategory +
                (1|Subject) + (1|Experiment) +
                (1|Item) + (1|Team),
              data = df, family = binomial,
              method = 'LRT')
xmdl_sub_concept
```

## Test of random effects

We can perform likelihood ratio tests (with restricted maximum likelihood) for the random effects.

```{r fit_models, cache = TRUE}
xmdl_nosub <- glmer(ACC ~ 1 + (1|Experiment) +
                (1|Item) + (1|Team),
              data = df, family = binomial)
xmdl_nolang <- glmer(ACC ~ 1 + (1|Subject) +
                (1|Item) + (1|Team),
              data = df, family = binomial)
xmdl_noitem <- glmer(ACC ~ 1 + (1|Subject) +
                (1|Experiment) + (1|Team),
              data = df, family = binomial)
xmdl_noteam <- glmer(ACC ~ 1 + (1|Subject) +
                (1|Experiment) + (1|Item),
              data = df, family = binomial)
```

Perform likelihood ratio test of random effects. This is likely anti-conservative given that the models have been fitted with maximum likelihood...

```{r RE_LRTs}

# Test of subject random effect:
anova(xmdl_nosub, xmdl)

# Test of language random effect:
anova(xmdl_nolang, xmdl)

# Test of item random effect:
anova(xmdl_noitem, xmdl)

# Test of team random effect:
anova(xmdl_noteam, xmdl)
```

All random effects are indicated to be significant.






