---
title: "German & Polish analysis"
author: "Bodo Winter"
date: "9/3/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preprocessing

Load libraries:

```{r libs, message = FALSE}
library(tidyverse) # for data carpentry
library(stringr) # for string processing
library(brms) # for bayesian models
```

Let's load in the data files:

```{r load_files, message = FALSE}
pol <- read_delim('../data/2018-08-21-polish.txt', delim = '\t')
ger <- read_delim('../data/2018-08-21-german.txt', delim = '\t')

# Check:

ger %>% print(width = Inf, n = 2)
```

Selected only those columns needed (for now):

```{r rid_cols}
ger <- select(ger,
              userid, itemid,
              firstlanguage, participantage, sex,
              position,
              expected, inputvalue,
              options,
              repetitions)
pol <- select(pol,
              userid, itemid,
              firstlanguage, participantage, sex,
              position,
              expected, inputvalue,
              options,
              repetitions)
```

Rename relevant columns for easier typing.

```{r renaming}
ger <- rename(ger,
              sub = userid,
              item = itemid,
              language = firstlanguage,
              age = participantage,
              input = inputvalue)
pol <- rename(pol,
              sub = userid,
              item = itemid,
              language = firstlanguage,
              age = participantage,
              input = inputvalue)
```

Next, I want to extract the relevant options bit. This is really ugly for now (will think of a more elegant regex one-liner soon). This is based on the assumption that the first item is the expected stimulus.

```{r extract_options}
# Extract response string by splitting at the bracket:

ger_resp <- str_split(ger$options, '\\[', simplify = TRUE)[, 2]
pol_resp <- str_split(pol$options, '\\[', simplify = TRUE)[, 2]

# Get rid of first two special characters:

ger_resp <- str_sub(ger_resp, 2, str_length(ger_resp))
pol_resp <- str_sub(pol_resp, 2, str_length(pol_resp))

# Take the word out:

suppressWarnings(ger$choice <- separate(tibble(ger_resp),
                                        ger_resp, into = LETTERS)[, 1]$A)
suppressWarnings(pol$choice <- separate(tibble(pol_resp),
                                        pol_resp, into = LETTERS)[, 1]$A)

## Let's get rid of the options column:

ger <- select(ger, -options)
pol <- select(pol, -options)

# Sanity-check: How does inputvalue versus choice look like?

select(ger, input, choice)
select(pol, input, choice)
```

Take only the native speakers of each language:

```{r nativespeakers}
# Check responses:

ger_all_langs <- unique(ger$language)
pol_all_langs <- unique(pol$language)
ger_all_langs
pol_all_langs

# Valid:

ger_speakers <- ger_all_langs[str_detect(ger_all_langs, '(d|D)eutsch')]
pol_speakers <- pol_all_langs[str_detect(pol_all_langs, '(p|P)olski')]

# Take only those:

ger <- filter(ger, language %in% ger_speakers)
pol <- filter(pol, language %in% pol_speakers)
```

Get rid of those that didn't complete the full experiment:

```{r incompletes}
# Table with how many data points there are per subject:

ger_subs <- table(ger$sub)
pol_subs <- table(pol$sub)

# Get those that do not have 104 data points:

bad_germans <- names(ger_subs)[which(ger_subs != 104)]
bad_poles <- names(pol_subs)[which(pol_subs != 104)]

## How many are there that need to be excluded?

length(bad_germans)
length(bad_poles)

## Out of how many in total?

length(ger_subs)
length(pol_subs)

## So that's how much exclusion?

round(length(bad_germans) / length(ger_subs), 2)
round(length(bad_poles) / length(pol_subs), 2)

# Extract them:

ger <- filter(ger, !(sub %in% bad_germans))
pol <- filter(pol, !(sub %in% bad_poles))
```

So, 38% of all Polish participants didn't complete the experiment, and 22% of all Germans.

Let's get rid of any subject that has gotten the clapping wrong at least once:

```{r clap_exclude}
# Subset of clapping items:

ger_claps <- filter(ger, choice == 'Klatschen')
pol_claps <- filter(pol, choice == 'klaskanie')

# Identifier for whether clap was identified correctly:

ger_claps <- mutate(ger_claps,
                    clapACC = ifelse(input == 'Klatschen', 1, 0))
pol_claps <- mutate(pol_claps,
                    clapACC = ifelse(input == 'klaskanie', 1, 0))

# Tabulate accuracy by subject:

ger_clap_subs <- ger_claps %>%
  group_by(sub) %>% 
  summarize(ACC = sum(clapACC))
pol_clap_subs <- pol_claps %>%
  group_by(sub) %>% 
  summarize(ACC = sum(clapACC))

# How many incorrect?

nrow(filter(ger_clap_subs, ACC != 10))
nrow(filter(pol_clap_subs, ACC != 10))

# Out of how many?

nrow(ger_clap_subs)
nrow(pol_clap_subs)

# Percentage:

nrow(filter(ger_clap_subs, ACC != 10)) / nrow(ger_clap_subs)
nrow(filter(pol_clap_subs, ACC != 10)) / nrow(pol_clap_subs)

# Get the subjects for which the claps were wrong:

bad_germans <- filter(ger_clap_subs, ACC != 10) %>% pull(sub)
bad_poles <- filter(pol_clap_subs, ACC != 10) %>% pull(sub)

# Extract the wrong-clappers:

ger <- filter(ger,
              !(sub %in% bad_germans))
pol <- filter(pol,
              !(sub %in% bad_poles))
```

So about 10% of the remaining people had to be excluded because they got the claps wrong.

Let's get rid of the kiki and bouba and r/l stims, as well as the clapping trials:

```{r kiki_r_l_extract}
# Define vector of trials to extract:

bad_trials <- c('r', 'l', 'kiki', 'bouba', 'klaskanie', 'Klatschen')

# Get rid of those trials:

ger <- filter(ger,
              !(choice %in% bad_trials))
pol <- filter(pol,
              !(choice %in% bad_trials))
```

Let's check whether this work. Everybody should have 90 data points now (3 * 30).

```{r check}
table(ger$sub)
table(pol$sub)
```

We now only have people with = 90 items.

Each item should occur in equal proportions:

```{r input_check}
table(ger$item)
table(pol$item)

# Check whether they are all the same to the number of subs:

all(table(ger$item) == length(unique(ger$sub)))
all(table(pol$item) == length(unique(pol$sub)))
```

That is the case. So it all looks good now.

## Check accuracy:

Let's create an accuracy measure, numerical with 0 = inaccurate and 1 = accurate.

```{r acc_create}
ger <- mutate(ger,
              ACC = ifelse(input == choice, 1, 0))
pol <- mutate(pol,
              ACC = ifelse(input == choice, 1, 0))
```

Let's check the average accuracy:

```{r avg_acc}
round(mean(ger$ACC), 2)
round(mean(pol$ACC), 2)
```

71% for Germans, 60% for Poles. So the with 1 out 6 options, the Poles are performing exactly at chance.

Let's check accuracy per concept:

```{r acc_per_concept}
ger %>% group_by(choice) %>%
  summarize(ACC = mean(ACC)) %>%
  mutate(ACC = round(ACC, 2)) %>% 
  arrange(desc(ACC)) %>% 
  print(n = Inf)
pol %>% group_by(choice) %>%
  summarize(ACC = mean(ACC)) %>%
  mutate(ACC = round(ACC, 2)) %>% 
  arrange(desc(ACC)) %>% 
  print(n = Inf)
```

Some of this is still suspicious (what's the empty one, and is schlafen actually exactly 100% correct?).

Check average per subject:

```{r avg_per_sub}
ger_subs <- ger %>% group_by(sub) %>%
  summarize(ACC = mean(ACC)) %>% 
  mutate(ACC = round(ACC, 2)) %>% 
  print(n = Inf)
pol_subs <- pol %>% group_by(sub) %>%
  summarize(ACC = mean(ACC)) %>% 
  mutate(ACC = round(ACC, 2)) %>% 
  print(n = Inf)
```

# First logistic regression model:

We want to look at the posterior of accuracy values, specifically at the intercept. If the mass of the posterior values are far away from the 60%, then we can conclude that participants performed the task with above-chance performance.

For now we will build separate models for German and Polish, but this will be done within one model later.

First, we set some options for Bayesian computation. This ensures that we'll use all cores from our processor so that MCMC chains can be run in parallel.

```{r parallel_cores}
options(mc.cores=parallel::detectCores())
```

Next, we set the prior for the intercept. We want a sufficiently wide prior that is centered at 60%, which is the chance baseline. So let's compute the logit value for 60% first.

```{r compute_logit}
qlogis(0.6)

# Save value:
my_logit <- qlogis(0.6)
```

If we set a normal prior on this baseline value with a +2/-2 standard deviation, then that's saying that we expect 95% of all responses to be between these two values.

```{r explore_priors}
plogis(0.6 - 2)
plogis(0.6 + 2)
```

So, if we take this prior, we are effectively saying that we assume that 95% of all responses are going to lie between average accuracy values of 20% and 93%, which seems reasonable.

```{r}
my_priors <- c(prior(normal(0.6, 2), class = 'Intercept'))
# Check:
my_priors
```

Fit the model for German:

```{r ger_bayes, cache = TRUE, message = FALSE}
ger.mdl <- brm(ACC ~ 1 + (1|sub) + (1|item),
               data = ger, family = bernoulli(), prior = my_priors,
               chains = 4, warmup = 2000, iter = 4000)
save(ger.mdl, file = '../models/ger.mdl')
pol.mdl <- brm(ACC ~ 1 + (1|sub) + (1|item),
               data = ger, family = bernoulli(), prior = my_priors,
               chains = 4, warmup = 2000, iter = 4000)
save(pol.mdl, file = '../models/pol.mdl')
# control = list(adapt_delta = 0.99))
```

Show the summaries:

```{r show_summaries}
summary(ger.mdl)
summary(pol.mdl)
```

Some of the Rhat values are worrisome (not exactly equals to 1.00), so this needs to be investigated.

To interpret this better, let's look at the probabilities that form the 95% credible interval.

```{r backtransform_probs}
# Posterior average (percentage), and lower and upper bound of 95% CI, German:
plogis(fixef(ger.mdl)[-2])
# Posterior average (percentage), and lower and upper bound of 95% CI, Polish:
plogis(fixef(pol.mdl)[-2])
```

Something's off (needs to be investigated) since both German and Polish have higher values than what is expected.

(Quick check with lme4)

```{r lme4}
library(lme4)
ger.lme4 <- glmer(ACC ~ 1 + (1|sub) + (1|item),
               data = ger, family = binomial)
summary(ger.lme4)
plogis(fixef(ger.lme4))
```

This model also is too high. I think this has to do with properties of the binomial, which may draw values away from a logit of 0. Will have to investigate this. So for now, let's do a quick check for German and Polish with some non-parametric tests (subjects-analysis only).

```{r ACC_sub_wilcox}
wilcox.test(ger_subs$ACC, mu = 0.6)
wilcox.test(pol_subs$ACC, mu = 0.6)
```

Based on this, the Germans perform significantly better than chance, but the Polish do not.

## 2do list:

* investigate MCMC estimation problems (also: trace plots)
* investigate logit boundary effects
* then, make plots of posteriors
* with Christopher: fix remaining issues in online script / make sure interpretation is correct



